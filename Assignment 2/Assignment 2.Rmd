---
title: "Assignment 2"
author: "Morris Chi"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: yes
    theme: spacelab
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


17/17

# 1

a. 7.88%

b. 25.45%

c. 23.6%

```{R}

P_E <- 5 / 10       
P_N <- 5 / 10       
P_F_given_E <- 0.0788  
P_F_given_N <- 0.2545  

P_F <- (P_F_given_E * P_E) + (P_F_given_N * P_N)


P_E_given_F <- (P_F_given_E * P_E) / P_F


P_E_given_F


```

# 2

a. 0.5

b. 0.99

c. 84.75%

```{R}
total_athletes <- 1000
users <- 100
nonusers <- total_athletes - users

sensitivity <- 0.95
false_positive_rate <- 0.13
specificity <- 1 - false_positive_rate

P_D <- users / total_athletes
P_D_complement <- nonusers / total_athletes

P_T_given_D <- sensitivity
P_T_given_D_complement <- false_positive_rate

P_T_given_D_complement
P_D_complement

P_T <- (P_T_given_D * P_D) + (P_T_given_D_complement * P_D_complement)

P_D_given_T <- (P_T_given_D * P_D) / P_T

cat("a. Sensitivity (Probability of testing positive given the athlete is a user):", sensitivity, "\n")
cat("b. Specificity (Probability of testing negative given the athlete is a nonuser):", specificity, "\n")
cat("c. Probability that the athlete is really doping given a positive test result (Positive Predictive Value):", P_D_given_T, "\n")
```

# 3

Pretty much, if you were to have a k number of sets of choices, then for each set, you have a new number of choices depending on how many choices are in each set, n1 for the first, n2 for the second, n3, and so on. However, to pick one of the choices in the first set, per say out of 3, then you still have the total number of choices in the second set, and the third set, and so on, so you would have 1 * n2 * n3 * ... * nk. However, there's 3 total choices in the first set, so then you would have (1 * n2 * n3 * ... * nk) * (1 * n2 * n3 * ... * nk) * (1 * n2 * n3 * ... * nk), or 3 * n2 * n3 * ... * nk. More generally, it would be n1 * n2 * n3 * ... * nk, as n1 would equal the number of choices within that first set. From there, it goes on, as each n1 or n2 or n3 and so on would each represent how many choices were in that specific set, because each choice must be chosen with every possible choice, one by one, starting from n1, then from n2, then from n3 and so on.

# 4

Essentially, if you want to find the number of different arrangements of a number of elements, (n), from a set consisting of a larger number of elements, (N), arranged in a specific order, you have to first find out how many options you have out of (n). You start with (N) elements, yet once you choose one to begin with, you now have (N-1) elements, one fewer than you started with. Then, as you go down the list, you move to (N-2) options, then (N-3), continuing on until you fill up all (n) options, leaving you with (N - n + 1) options left. The total number of all these choices is every option multiplied together, (N) * (N-1) * (N-2) * ... * (N - n + 1), which results in a factorial. This total product can be shown in a fraction of factorials, where on top you have (N)!, being all possible options from (N) to 1, and on the bottom you have (N - n)!, being all other options from (N - n) all the way to 1. With this fraction, you take all possible options and then remove the ones outside of the range of (N - n), thus removing the ones you don't want, since you want to find out how many options there are of (n), and only leaving you with the factorial of all the options that you do want that make up the number of different arrangments of a number of elements (n).

# 5

So, starting with N number of elements, it has N! ways to order all of the N elements. However, it needs to be divided into k number of groups, which would typically be by dividing the total by the number in which it needs to be separated, like 10 cars between 5 people, (10/5). As such, N needs to be divided by k number of groups, so (N/n1 * n2 * ... * nk), but there's N! many ways N elements can be arranged, but having N! is way too big to be divided by just n1 * ... * nk, and the order of the elements doesn't matter inside of the groups. So, n1 becomes n1!, which is the number of ways n1 elements can be sorted inside of the group, and so do all of the other n's, up until nk!. This makes it so that the total ways that the N elements can be ordered is divided into k groups by the number of ways each n elements can be sorted inside of its respective group, or every way N elements can be divided into k groups. This turns the equation into (N!/n1! * n2! * ... * nk!).

# 6

So, if you have N total elements, but you only want to pick n of the elements, initially you have the permutation of P(N,n) = N!/(N - n)!, which goes through every N choice until all n elements are picked. However, the order that the n elements are selected doesn't matter, just that they do get selected, which means that this can be reduced. Since every group of the n elements can be arranged in n! different ways, then that can be removed from the original permutation as it is unnecessary, so the permutation becomes P(N, n) = N!/n!(N - n)!. So that shows the number of ways you can choose n items from N.


# 7

a. 1

b. 0.24

c. 0.39


```{R}
p <- c(0.09, 0.30, 0.37, 0.20, 0.04)

sum(p)

prob_three_or_four <- p[4] + p[5]
prob_three_or_four

prob_less_than_two <- p[1] + p[2]
prob_less_than_two
```

# 8 

a. The sum equals 1

b. 0.145

c. Mean: 4.38   Variance: 15/14

d. [0, 9]


```{R}
y <- 0:20
p <- c(0.17, 0.10, 0.11, 0.11, 0.10, 0.10, 0.07, 0.05, 0.03, 0.02,
        0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.005, 0.005)

sum(p)

p_geq_10 <- sum(p[11:21])
p_geq_10

mean_Y <- sum(y * p)  
variance_Y <- sum((y - mean_Y)^2 * p)
mean_Y
variance_Y

cdf <- cumsum(p)  
interval_start <- which(cdf >= 0.125)[1] - 1
interval_end <- which(cdf >= 0.875)[1] - 1
interval_start
interval_end


```

# 9

a. 0.0013

b. 3.46e-07

c. Mean: 17.5   Standard Deviation: 2.29

d. On average, 17.5 of the students are foreign nationals, but the actual number varies by 2.29


```{R}
n <- 25
p <- 0.70

p_Y_10 <- dbinom(10, size = n, prob = p)
p_Y_10

p_Y_leq_5 <- pbinom(5, size = n, prob = p)
p_Y_leq_5

mean_Y <- n * p
sd_Y <- sqrt(n * p * (1 - p))
mean_Y
sd_Y


```

# 10

a. 4.91e-07

b. 0.0338

```{R}
n <- 50
k <- 10
p <- 1 / k

p_equal <- dmultinom(rep(5, k), size = n, prob = rep(p, k))
p_equal

p_underutilized <- pbinom(1, size = n, prob = p)
p_underutilized


```

# 11

a. Negative Binomial Distribution   P(Y = y) = (y-1/4)(0.15)^5(0.85)^y-5

b. 0.163%

```{R}

y <- 8
p_failure <- 0.15
p_no_flaw <- 0.85


probability_Y_8 <- choose(y - 1, 4) * (p_failure^5) * (p_no_flaw^(y - 5))
probability_Y_8

```

# 12 

a. 0.383

b. 0.0169%

```{R}

K <- 8   
N <- 209 
n <- 10 
x <- 4

expected_value <- (K * n) / N

probability_4 <- dhyper(x, K, N - K, n)

expected_value

probability_4
```

# 13

a. 0.03

b. The number of causualties should be independent across the ships, as well as the averate rate of 0.03 being constant over the 3 years across the ships. Also, the casualties should be rare events on the ships they are on. If these are true, then the researcher's Poisson assumption will be  plausible

c. 0.97


```{R}
lambda <- 0.03
variance <- lambda
variance


p_no_casualties <- dpois(0, lambda)
p_no_casualties

```


# 14

a. 2/3

b. F(y) = 0 if y < 0, (4y - y^2)/3 if 0 ≤ y ≤ 1, and 1 if y > 1

c. 0.48

d. 0.55


```{R}

c <- 2/3
c

F <- function(y) {
  ifelse(y < 0, 0,
         ifelse(y > 1, 1,
                (4*y - y^2)/3))
}

F_04 <- F(0.4)
F_04


P <- F(0.6) - F(0.1)
P
```

# 15

a. 0, 5

b. 0, 0.0014

c. 0, 18000



```{R}
f <- function(y) {
  ifelse(y > -5 & y < 5, (3/500)*(25 - y^2), 0)
}

EY <- integrate(function(y) y * f(y), -5, 5)$value
EY

EY2 <- integrate(function(y) y^2 * f(y), -5, 5)$value
VarY <- EY2 - EY^2
VarY


mean_hours <- EY / 60
var_hours  <- VarY / 60^2


mean_sec <- EY * 60
var_sec  <- VarY * 60^2

mean_hours; var_hours
mean_sec; var_sec


```

# 16

a. 94.1%

b. 94.1%

c. 11.1%

```{R}
mu <- 50   
sigma <- 3.2  

z_a <- (45 - mu) / sigma
P_a <- 1 - pnorm(z_a)
cat("a. Probability that the alkalinity level exceeds 45 mg/L:", P_a, "\n")

z_b <- (55 - mu) / sigma
P_b <- pnorm(z_b)
cat("b. Probability that the alkalinity level is below 55 mg/L:", P_b, "\n")

z_c1 <- (51 - mu) / sigma
z_c2 <- (52 - mu) / sigma
P_c <- pnorm(z_c2) - pnorm(z_c1)
cat("c. Probability that the alkalinity level is between 51 and 52 mg/L:", P_c, "\n")
```

# 17

a. 0.411

b. 0.1513

c. 0.9073

d. 0.0164



```{R}
mu <- 605
sigma <- 185

p_a <- pnorm(700, mean = mu, sd = sigma) - pnorm(500, mean = mu, sd = sigma)
p_a

p_b <- pnorm(500, mean = mu, sd = sigma) - pnorm(400, mean = mu, sd = sigma)
p_b

p_c <- pnorm(850, mean = mu, sd = sigma)
p_c

p_d <- 1 - pnorm(1000, mean = mu, sd = sigma)
p_d
```

